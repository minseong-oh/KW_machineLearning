2025-06-08 18:08:02,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-08 18:08:02,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-08 18:08:02,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-08 18:08:02,801:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-08 18:10:22,151:INFO:PyCaret ClassificationExperiment
2025-06-08 18:10:22,151:INFO:Logging name: clf-default-name
2025-06-08 18:10:22,151:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-08 18:10:22,151:INFO:version 3.3.2
2025-06-08 18:10:22,151:INFO:Initializing setup()
2025-06-08 18:10:22,151:INFO:self.USI: 4afc
2025-06-08 18:10:22,151:INFO:self._variable_keys: {'data', 'gpu_n_jobs_param', 'pipeline', 'y_train', 'fold_generator', 'USI', '_ml_usecase', 'X', 'y_test', 'n_jobs_param', 'idx', 'exp_name_log', 'memory', 'html_param', 'gpu_param', 'X_test', 'fold_shuffle_param', 'exp_id', 'y', 'seed', 'fold_groups_param', '_available_plots', 'target_param', 'fix_imbalance', 'is_multiclass', 'log_plots_param', 'logging_param', 'X_train'}
2025-06-08 18:10:22,151:INFO:Checking environment
2025-06-08 18:10:22,151:INFO:python_version: 3.10.16
2025-06-08 18:10:22,151:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-06-08 18:10:22,151:INFO:machine: AMD64
2025-06-08 18:10:22,151:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-08 18:10:22,156:INFO:Memory: svmem(total=33885192192, available=19223142400, percent=43.3, used=14662049792, free=19223142400)
2025-06-08 18:10:22,156:INFO:Physical Core: 14
2025-06-08 18:10:22,156:INFO:Logical Core: 18
2025-06-08 18:10:22,156:INFO:Checking libraries
2025-06-08 18:10:22,156:INFO:System:
2025-06-08 18:10:22,156:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-06-08 18:10:22,156:INFO:executable: C:\Users\omslu\anaconda3\envs\pycaret_env\python.exe
2025-06-08 18:10:22,156:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-08 18:10:22,156:INFO:PyCaret required dependencies:
2025-06-08 18:10:22,188:INFO:                 pip: 25.1
2025-06-08 18:10:22,188:INFO:          setuptools: 78.1.1
2025-06-08 18:10:22,188:INFO:             pycaret: 3.3.2
2025-06-08 18:10:22,188:INFO:             IPython: 8.37.0
2025-06-08 18:10:22,188:INFO:          ipywidgets: 8.1.7
2025-06-08 18:10:22,188:INFO:                tqdm: 4.67.1
2025-06-08 18:10:22,188:INFO:               numpy: 1.26.4
2025-06-08 18:10:22,188:INFO:              pandas: 2.1.4
2025-06-08 18:10:22,188:INFO:              jinja2: 3.1.6
2025-06-08 18:10:22,188:INFO:               scipy: 1.11.4
2025-06-08 18:10:22,190:INFO:              joblib: 1.3.2
2025-06-08 18:10:22,190:INFO:             sklearn: 1.4.2
2025-06-08 18:10:22,190:INFO:                pyod: 2.0.5
2025-06-08 18:10:22,190:INFO:            imblearn: 0.13.0
2025-06-08 18:10:22,190:INFO:   category_encoders: 2.7.0
2025-06-08 18:10:22,190:INFO:            lightgbm: 4.6.0
2025-06-08 18:10:22,190:INFO:               numba: 0.61.2
2025-06-08 18:10:22,190:INFO:            requests: 2.32.3
2025-06-08 18:10:22,190:INFO:          matplotlib: 3.7.5
2025-06-08 18:10:22,190:INFO:          scikitplot: 0.3.7
2025-06-08 18:10:22,190:INFO:         yellowbrick: 1.5
2025-06-08 18:10:22,190:INFO:              plotly: 5.24.1
2025-06-08 18:10:22,190:INFO:    plotly-resampler: Not installed
2025-06-08 18:10:22,190:INFO:             kaleido: 0.2.1
2025-06-08 18:10:22,190:INFO:           schemdraw: 0.15
2025-06-08 18:10:22,190:INFO:         statsmodels: 0.14.4
2025-06-08 18:10:22,190:INFO:              sktime: 0.26.0
2025-06-08 18:10:22,190:INFO:               tbats: 1.1.3
2025-06-08 18:10:22,190:INFO:            pmdarima: 2.0.4
2025-06-08 18:10:22,190:INFO:              psutil: 7.0.0
2025-06-08 18:10:22,190:INFO:          markupsafe: 3.0.2
2025-06-08 18:10:22,190:INFO:             pickle5: Not installed
2025-06-08 18:10:22,190:INFO:         cloudpickle: 3.1.1
2025-06-08 18:10:22,190:INFO:         deprecation: 2.1.0
2025-06-08 18:10:22,190:INFO:              xxhash: 3.5.0
2025-06-08 18:10:22,190:INFO:           wurlitzer: Not installed
2025-06-08 18:10:22,190:INFO:PyCaret optional dependencies:
2025-06-08 18:10:22,326:INFO:                shap: 0.47.2
2025-06-08 18:10:22,326:INFO:           interpret: Not installed
2025-06-08 18:10:22,326:INFO:                umap: Not installed
2025-06-08 18:10:22,328:INFO:     ydata_profiling: Not installed
2025-06-08 18:10:22,328:INFO:  explainerdashboard: Not installed
2025-06-08 18:10:22,328:INFO:             autoviz: Not installed
2025-06-08 18:10:22,328:INFO:           fairlearn: Not installed
2025-06-08 18:10:22,328:INFO:          deepchecks: Not installed
2025-06-08 18:10:22,328:INFO:             xgboost: 3.0.2
2025-06-08 18:10:22,328:INFO:            catboost: Not installed
2025-06-08 18:10:22,328:INFO:              kmodes: Not installed
2025-06-08 18:10:22,328:INFO:             mlxtend: Not installed
2025-06-08 18:10:22,328:INFO:       statsforecast: Not installed
2025-06-08 18:10:22,328:INFO:        tune_sklearn: Not installed
2025-06-08 18:10:22,328:INFO:                 ray: Not installed
2025-06-08 18:10:22,328:INFO:            hyperopt: Not installed
2025-06-08 18:10:22,328:INFO:              optuna: Not installed
2025-06-08 18:10:22,330:INFO:               skopt: Not installed
2025-06-08 18:10:22,330:INFO:              mlflow: Not installed
2025-06-08 18:10:22,330:INFO:              gradio: Not installed
2025-06-08 18:10:22,330:INFO:             fastapi: Not installed
2025-06-08 18:10:22,330:INFO:             uvicorn: Not installed
2025-06-08 18:10:22,330:INFO:              m2cgen: Not installed
2025-06-08 18:10:22,330:INFO:           evidently: Not installed
2025-06-08 18:10:22,330:INFO:               fugue: Not installed
2025-06-08 18:10:22,330:INFO:           streamlit: Not installed
2025-06-08 18:10:22,330:INFO:             prophet: Not installed
2025-06-08 18:10:22,330:INFO:None
2025-06-08 18:10:22,330:INFO:Set up data.
2025-06-08 18:10:22,347:INFO:Set up folding strategy.
2025-06-08 18:10:22,349:INFO:Set up train/test split.
2025-06-08 18:10:34,177:INFO:PyCaret ClassificationExperiment
2025-06-08 18:10:34,177:INFO:Logging name: clf-default-name
2025-06-08 18:10:34,177:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-08 18:10:34,177:INFO:version 3.3.2
2025-06-08 18:10:34,177:INFO:Initializing setup()
2025-06-08 18:10:34,178:INFO:self.USI: 2fe8
2025-06-08 18:10:34,178:INFO:self._variable_keys: {'data', 'gpu_n_jobs_param', 'pipeline', 'y_train', 'fold_generator', 'USI', '_ml_usecase', 'X', 'y_test', 'n_jobs_param', 'idx', 'exp_name_log', 'memory', 'html_param', 'gpu_param', 'X_test', 'fold_shuffle_param', 'exp_id', 'y', 'seed', 'fold_groups_param', '_available_plots', 'target_param', 'fix_imbalance', 'is_multiclass', 'log_plots_param', 'logging_param', 'X_train'}
2025-06-08 18:10:34,178:INFO:Checking environment
2025-06-08 18:10:34,178:INFO:python_version: 3.10.16
2025-06-08 18:10:34,178:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-06-08 18:10:34,178:INFO:machine: AMD64
2025-06-08 18:10:34,178:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-08 18:10:34,182:INFO:Memory: svmem(total=33885192192, available=18887593984, percent=44.3, used=14997598208, free=18887593984)
2025-06-08 18:10:34,183:INFO:Physical Core: 14
2025-06-08 18:10:34,183:INFO:Logical Core: 18
2025-06-08 18:10:34,183:INFO:Checking libraries
2025-06-08 18:10:34,183:INFO:System:
2025-06-08 18:10:34,183:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-06-08 18:10:34,183:INFO:executable: C:\Users\omslu\anaconda3\envs\pycaret_env\python.exe
2025-06-08 18:10:34,183:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-08 18:10:34,183:INFO:PyCaret required dependencies:
2025-06-08 18:10:34,183:INFO:                 pip: 25.1
2025-06-08 18:10:34,183:INFO:          setuptools: 78.1.1
2025-06-08 18:10:34,183:INFO:             pycaret: 3.3.2
2025-06-08 18:10:34,183:INFO:             IPython: 8.37.0
2025-06-08 18:10:34,183:INFO:          ipywidgets: 8.1.7
2025-06-08 18:10:34,183:INFO:                tqdm: 4.67.1
2025-06-08 18:10:34,183:INFO:               numpy: 1.26.4
2025-06-08 18:10:34,183:INFO:              pandas: 2.1.4
2025-06-08 18:10:34,183:INFO:              jinja2: 3.1.6
2025-06-08 18:10:34,183:INFO:               scipy: 1.11.4
2025-06-08 18:10:34,184:INFO:              joblib: 1.3.2
2025-06-08 18:10:34,184:INFO:             sklearn: 1.4.2
2025-06-08 18:10:34,184:INFO:                pyod: 2.0.5
2025-06-08 18:10:34,184:INFO:            imblearn: 0.13.0
2025-06-08 18:10:34,184:INFO:   category_encoders: 2.7.0
2025-06-08 18:10:34,184:INFO:            lightgbm: 4.6.0
2025-06-08 18:10:34,184:INFO:               numba: 0.61.2
2025-06-08 18:10:34,184:INFO:            requests: 2.32.3
2025-06-08 18:10:34,184:INFO:          matplotlib: 3.7.5
2025-06-08 18:10:34,184:INFO:          scikitplot: 0.3.7
2025-06-08 18:10:34,184:INFO:         yellowbrick: 1.5
2025-06-08 18:10:34,184:INFO:              plotly: 5.24.1
2025-06-08 18:10:34,184:INFO:    plotly-resampler: Not installed
2025-06-08 18:10:34,184:INFO:             kaleido: 0.2.1
2025-06-08 18:10:34,184:INFO:           schemdraw: 0.15
2025-06-08 18:10:34,184:INFO:         statsmodels: 0.14.4
2025-06-08 18:10:34,184:INFO:              sktime: 0.26.0
2025-06-08 18:10:34,184:INFO:               tbats: 1.1.3
2025-06-08 18:10:34,184:INFO:            pmdarima: 2.0.4
2025-06-08 18:10:34,184:INFO:              psutil: 7.0.0
2025-06-08 18:10:34,184:INFO:          markupsafe: 3.0.2
2025-06-08 18:10:34,184:INFO:             pickle5: Not installed
2025-06-08 18:10:34,184:INFO:         cloudpickle: 3.1.1
2025-06-08 18:10:34,184:INFO:         deprecation: 2.1.0
2025-06-08 18:10:34,184:INFO:              xxhash: 3.5.0
2025-06-08 18:10:34,184:INFO:           wurlitzer: Not installed
2025-06-08 18:10:34,184:INFO:PyCaret optional dependencies:
2025-06-08 18:10:34,184:INFO:                shap: 0.47.2
2025-06-08 18:10:34,184:INFO:           interpret: Not installed
2025-06-08 18:10:34,184:INFO:                umap: Not installed
2025-06-08 18:10:34,184:INFO:     ydata_profiling: Not installed
2025-06-08 18:10:34,184:INFO:  explainerdashboard: Not installed
2025-06-08 18:10:34,184:INFO:             autoviz: Not installed
2025-06-08 18:10:34,184:INFO:           fairlearn: Not installed
2025-06-08 18:10:34,184:INFO:          deepchecks: Not installed
2025-06-08 18:10:34,185:INFO:             xgboost: 3.0.2
2025-06-08 18:10:34,185:INFO:            catboost: Not installed
2025-06-08 18:10:34,185:INFO:              kmodes: Not installed
2025-06-08 18:10:34,185:INFO:             mlxtend: Not installed
2025-06-08 18:10:34,185:INFO:       statsforecast: Not installed
2025-06-08 18:10:34,185:INFO:        tune_sklearn: Not installed
2025-06-08 18:10:34,185:INFO:                 ray: Not installed
2025-06-08 18:10:34,185:INFO:            hyperopt: Not installed
2025-06-08 18:10:34,185:INFO:              optuna: Not installed
2025-06-08 18:10:34,185:INFO:               skopt: Not installed
2025-06-08 18:10:34,185:INFO:              mlflow: Not installed
2025-06-08 18:10:34,185:INFO:              gradio: Not installed
2025-06-08 18:10:34,185:INFO:             fastapi: Not installed
2025-06-08 18:10:34,185:INFO:             uvicorn: Not installed
2025-06-08 18:10:34,185:INFO:              m2cgen: Not installed
2025-06-08 18:10:34,185:INFO:           evidently: Not installed
2025-06-08 18:10:34,185:INFO:               fugue: Not installed
2025-06-08 18:10:34,185:INFO:           streamlit: Not installed
2025-06-08 18:10:34,185:INFO:             prophet: Not installed
2025-06-08 18:10:34,185:INFO:None
2025-06-08 18:10:34,185:INFO:Set up data.
2025-06-08 18:10:34,191:INFO:Set up folding strategy.
2025-06-08 18:10:34,192:INFO:Set up train/test split.
2025-06-08 18:11:01,783:INFO:PyCaret ClassificationExperiment
2025-06-08 18:11:01,783:INFO:Logging name: clf-default-name
2025-06-08 18:11:01,783:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-08 18:11:01,783:INFO:version 3.3.2
2025-06-08 18:11:01,783:INFO:Initializing setup()
2025-06-08 18:11:01,783:INFO:self.USI: b991
2025-06-08 18:11:01,783:INFO:self._variable_keys: {'data', 'gpu_n_jobs_param', 'pipeline', 'y_train', 'fold_generator', 'USI', '_ml_usecase', 'X', 'y_test', 'n_jobs_param', 'idx', 'exp_name_log', 'memory', 'html_param', 'gpu_param', 'X_test', 'fold_shuffle_param', 'exp_id', 'y', 'seed', 'fold_groups_param', '_available_plots', 'target_param', 'fix_imbalance', 'is_multiclass', 'log_plots_param', 'logging_param', 'X_train'}
2025-06-08 18:11:01,783:INFO:Checking environment
2025-06-08 18:11:01,783:INFO:python_version: 3.10.16
2025-06-08 18:11:01,783:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-06-08 18:11:01,783:INFO:machine: AMD64
2025-06-08 18:11:01,784:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-08 18:11:01,787:INFO:Memory: svmem(total=33885192192, available=18903953408, percent=44.2, used=14981238784, free=18903953408)
2025-06-08 18:11:01,787:INFO:Physical Core: 14
2025-06-08 18:11:01,787:INFO:Logical Core: 18
2025-06-08 18:11:01,787:INFO:Checking libraries
2025-06-08 18:11:01,787:INFO:System:
2025-06-08 18:11:01,787:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-06-08 18:11:01,787:INFO:executable: C:\Users\omslu\anaconda3\envs\pycaret_env\python.exe
2025-06-08 18:11:01,787:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-08 18:11:01,787:INFO:PyCaret required dependencies:
2025-06-08 18:11:01,787:INFO:                 pip: 25.1
2025-06-08 18:11:01,787:INFO:          setuptools: 78.1.1
2025-06-08 18:11:01,787:INFO:             pycaret: 3.3.2
2025-06-08 18:11:01,787:INFO:             IPython: 8.37.0
2025-06-08 18:11:01,787:INFO:          ipywidgets: 8.1.7
2025-06-08 18:11:01,787:INFO:                tqdm: 4.67.1
2025-06-08 18:11:01,787:INFO:               numpy: 1.26.4
2025-06-08 18:11:01,787:INFO:              pandas: 2.1.4
2025-06-08 18:11:01,787:INFO:              jinja2: 3.1.6
2025-06-08 18:11:01,787:INFO:               scipy: 1.11.4
2025-06-08 18:11:01,787:INFO:              joblib: 1.3.2
2025-06-08 18:11:01,789:INFO:             sklearn: 1.4.2
2025-06-08 18:11:01,789:INFO:                pyod: 2.0.5
2025-06-08 18:11:01,789:INFO:            imblearn: 0.13.0
2025-06-08 18:11:01,789:INFO:   category_encoders: 2.7.0
2025-06-08 18:11:01,789:INFO:            lightgbm: 4.6.0
2025-06-08 18:11:01,789:INFO:               numba: 0.61.2
2025-06-08 18:11:01,789:INFO:            requests: 2.32.3
2025-06-08 18:11:01,789:INFO:          matplotlib: 3.7.5
2025-06-08 18:11:01,789:INFO:          scikitplot: 0.3.7
2025-06-08 18:11:01,789:INFO:         yellowbrick: 1.5
2025-06-08 18:11:01,789:INFO:              plotly: 5.24.1
2025-06-08 18:11:01,789:INFO:    plotly-resampler: Not installed
2025-06-08 18:11:01,789:INFO:             kaleido: 0.2.1
2025-06-08 18:11:01,789:INFO:           schemdraw: 0.15
2025-06-08 18:11:01,789:INFO:         statsmodels: 0.14.4
2025-06-08 18:11:01,789:INFO:              sktime: 0.26.0
2025-06-08 18:11:01,789:INFO:               tbats: 1.1.3
2025-06-08 18:11:01,789:INFO:            pmdarima: 2.0.4
2025-06-08 18:11:01,789:INFO:              psutil: 7.0.0
2025-06-08 18:11:01,789:INFO:          markupsafe: 3.0.2
2025-06-08 18:11:01,789:INFO:             pickle5: Not installed
2025-06-08 18:11:01,790:INFO:         cloudpickle: 3.1.1
2025-06-08 18:11:01,790:INFO:         deprecation: 2.1.0
2025-06-08 18:11:01,790:INFO:              xxhash: 3.5.0
2025-06-08 18:11:01,790:INFO:           wurlitzer: Not installed
2025-06-08 18:11:01,790:INFO:PyCaret optional dependencies:
2025-06-08 18:11:01,790:INFO:                shap: 0.47.2
2025-06-08 18:11:01,790:INFO:           interpret: Not installed
2025-06-08 18:11:01,790:INFO:                umap: Not installed
2025-06-08 18:11:01,790:INFO:     ydata_profiling: Not installed
2025-06-08 18:11:01,790:INFO:  explainerdashboard: Not installed
2025-06-08 18:11:01,790:INFO:             autoviz: Not installed
2025-06-08 18:11:01,790:INFO:           fairlearn: Not installed
2025-06-08 18:11:01,790:INFO:          deepchecks: Not installed
2025-06-08 18:11:01,790:INFO:             xgboost: 3.0.2
2025-06-08 18:11:01,790:INFO:            catboost: Not installed
2025-06-08 18:11:01,790:INFO:              kmodes: Not installed
2025-06-08 18:11:01,790:INFO:             mlxtend: Not installed
2025-06-08 18:11:01,790:INFO:       statsforecast: Not installed
2025-06-08 18:11:01,790:INFO:        tune_sklearn: Not installed
2025-06-08 18:11:01,790:INFO:                 ray: Not installed
2025-06-08 18:11:01,790:INFO:            hyperopt: Not installed
2025-06-08 18:11:01,790:INFO:              optuna: Not installed
2025-06-08 18:11:01,790:INFO:               skopt: Not installed
2025-06-08 18:11:01,790:INFO:              mlflow: Not installed
2025-06-08 18:11:01,790:INFO:              gradio: Not installed
2025-06-08 18:11:01,790:INFO:             fastapi: Not installed
2025-06-08 18:11:01,790:INFO:             uvicorn: Not installed
2025-06-08 18:11:01,790:INFO:              m2cgen: Not installed
2025-06-08 18:11:01,790:INFO:           evidently: Not installed
2025-06-08 18:11:01,790:INFO:               fugue: Not installed
2025-06-08 18:11:01,790:INFO:           streamlit: Not installed
2025-06-08 18:11:01,790:INFO:             prophet: Not installed
2025-06-08 18:11:01,790:INFO:None
2025-06-08 18:11:01,791:INFO:Set up data.
2025-06-08 18:11:01,796:INFO:Set up folding strategy.
2025-06-08 18:11:01,796:INFO:Set up train/test split.
2025-06-08 18:12:02,385:INFO:PyCaret ClassificationExperiment
2025-06-08 18:12:02,385:INFO:Logging name: clf-default-name
2025-06-08 18:12:02,386:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-08 18:12:02,386:INFO:version 3.3.2
2025-06-08 18:12:02,386:INFO:Initializing setup()
2025-06-08 18:12:02,386:INFO:self.USI: d5ac
2025-06-08 18:12:02,386:INFO:self._variable_keys: {'data', 'gpu_n_jobs_param', 'pipeline', 'y_train', 'fold_generator', 'USI', '_ml_usecase', 'X', 'y_test', 'n_jobs_param', 'idx', 'exp_name_log', 'memory', 'html_param', 'gpu_param', 'X_test', 'fold_shuffle_param', 'exp_id', 'y', 'seed', 'fold_groups_param', '_available_plots', 'target_param', 'fix_imbalance', 'is_multiclass', 'log_plots_param', 'logging_param', 'X_train'}
2025-06-08 18:12:02,386:INFO:Checking environment
2025-06-08 18:12:02,386:INFO:python_version: 3.10.16
2025-06-08 18:12:02,386:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-06-08 18:12:02,386:INFO:machine: AMD64
2025-06-08 18:12:02,386:INFO:platform: Windows-10-10.0.26100-SP0
2025-06-08 18:12:02,390:INFO:Memory: svmem(total=33885192192, available=18941636608, percent=44.1, used=14943555584, free=18941636608)
2025-06-08 18:12:02,390:INFO:Physical Core: 14
2025-06-08 18:12:02,390:INFO:Logical Core: 18
2025-06-08 18:12:02,390:INFO:Checking libraries
2025-06-08 18:12:02,390:INFO:System:
2025-06-08 18:12:02,391:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-06-08 18:12:02,391:INFO:executable: C:\Users\omslu\anaconda3\envs\pycaret_env\python.exe
2025-06-08 18:12:02,391:INFO:   machine: Windows-10-10.0.26100-SP0
2025-06-08 18:12:02,391:INFO:PyCaret required dependencies:
2025-06-08 18:12:02,391:INFO:                 pip: 25.1
2025-06-08 18:12:02,391:INFO:          setuptools: 78.1.1
2025-06-08 18:12:02,391:INFO:             pycaret: 3.3.2
2025-06-08 18:12:02,391:INFO:             IPython: 8.37.0
2025-06-08 18:12:02,391:INFO:          ipywidgets: 8.1.7
2025-06-08 18:12:02,391:INFO:                tqdm: 4.67.1
2025-06-08 18:12:02,391:INFO:               numpy: 1.26.4
2025-06-08 18:12:02,391:INFO:              pandas: 2.1.4
2025-06-08 18:12:02,391:INFO:              jinja2: 3.1.6
2025-06-08 18:12:02,391:INFO:               scipy: 1.11.4
2025-06-08 18:12:02,391:INFO:              joblib: 1.3.2
2025-06-08 18:12:02,391:INFO:             sklearn: 1.4.2
2025-06-08 18:12:02,391:INFO:                pyod: 2.0.5
2025-06-08 18:12:02,391:INFO:            imblearn: 0.13.0
2025-06-08 18:12:02,391:INFO:   category_encoders: 2.7.0
2025-06-08 18:12:02,391:INFO:            lightgbm: 4.6.0
2025-06-08 18:12:02,391:INFO:               numba: 0.61.2
2025-06-08 18:12:02,391:INFO:            requests: 2.32.3
2025-06-08 18:12:02,391:INFO:          matplotlib: 3.7.5
2025-06-08 18:12:02,391:INFO:          scikitplot: 0.3.7
2025-06-08 18:12:02,391:INFO:         yellowbrick: 1.5
2025-06-08 18:12:02,391:INFO:              plotly: 5.24.1
2025-06-08 18:12:02,391:INFO:    plotly-resampler: Not installed
2025-06-08 18:12:02,391:INFO:             kaleido: 0.2.1
2025-06-08 18:12:02,391:INFO:           schemdraw: 0.15
2025-06-08 18:12:02,391:INFO:         statsmodels: 0.14.4
2025-06-08 18:12:02,393:INFO:              sktime: 0.26.0
2025-06-08 18:12:02,393:INFO:               tbats: 1.1.3
2025-06-08 18:12:02,393:INFO:            pmdarima: 2.0.4
2025-06-08 18:12:02,393:INFO:              psutil: 7.0.0
2025-06-08 18:12:02,393:INFO:          markupsafe: 3.0.2
2025-06-08 18:12:02,393:INFO:             pickle5: Not installed
2025-06-08 18:12:02,393:INFO:         cloudpickle: 3.1.1
2025-06-08 18:12:02,393:INFO:         deprecation: 2.1.0
2025-06-08 18:12:02,393:INFO:              xxhash: 3.5.0
2025-06-08 18:12:02,393:INFO:           wurlitzer: Not installed
2025-06-08 18:12:02,393:INFO:PyCaret optional dependencies:
2025-06-08 18:12:02,393:INFO:                shap: 0.47.2
2025-06-08 18:12:02,393:INFO:           interpret: Not installed
2025-06-08 18:12:02,393:INFO:                umap: Not installed
2025-06-08 18:12:02,393:INFO:     ydata_profiling: Not installed
2025-06-08 18:12:02,393:INFO:  explainerdashboard: Not installed
2025-06-08 18:12:02,393:INFO:             autoviz: Not installed
2025-06-08 18:12:02,393:INFO:           fairlearn: Not installed
2025-06-08 18:12:02,393:INFO:          deepchecks: Not installed
2025-06-08 18:12:02,393:INFO:             xgboost: 3.0.2
2025-06-08 18:12:02,393:INFO:            catboost: Not installed
2025-06-08 18:12:02,393:INFO:              kmodes: Not installed
2025-06-08 18:12:02,393:INFO:             mlxtend: Not installed
2025-06-08 18:12:02,393:INFO:       statsforecast: Not installed
2025-06-08 18:12:02,393:INFO:        tune_sklearn: Not installed
2025-06-08 18:12:02,393:INFO:                 ray: Not installed
2025-06-08 18:12:02,393:INFO:            hyperopt: Not installed
2025-06-08 18:12:02,393:INFO:              optuna: Not installed
2025-06-08 18:12:02,393:INFO:               skopt: Not installed
2025-06-08 18:12:02,393:INFO:              mlflow: Not installed
2025-06-08 18:12:02,393:INFO:              gradio: Not installed
2025-06-08 18:12:02,393:INFO:             fastapi: Not installed
2025-06-08 18:12:02,393:INFO:             uvicorn: Not installed
2025-06-08 18:12:02,393:INFO:              m2cgen: Not installed
2025-06-08 18:12:02,393:INFO:           evidently: Not installed
2025-06-08 18:12:02,393:INFO:               fugue: Not installed
2025-06-08 18:12:02,393:INFO:           streamlit: Not installed
2025-06-08 18:12:02,393:INFO:             prophet: Not installed
2025-06-08 18:12:02,393:INFO:None
2025-06-08 18:12:02,394:INFO:Set up data.
2025-06-08 18:12:02,404:INFO:Set up folding strategy.
2025-06-08 18:12:02,404:INFO:Set up train/test split.
2025-06-08 18:12:02,427:INFO:Set up index.
2025-06-08 18:12:02,442:INFO:Assigning column types.
2025-06-08 18:12:02,460:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-08 18:12:02,627:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-08 18:12:02,644:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-08 18:12:02,771:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:02,787:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:02,946:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-08 18:12:02,946:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-08 18:12:03,041:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:03,057:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:03,057:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-08 18:12:03,213:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-08 18:12:03,516:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:03,525:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:03,683:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-08 18:12:03,776:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:03,784:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:03,785:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-08 18:12:04,019:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:04,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:04,270:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:04,270:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:04,293:INFO:Preparing preprocessing pipeline...
2025-06-08 18:12:04,302:INFO:Set up simple imputation.
2025-06-08 18:12:04,440:INFO:Finished creating preprocessing pipeline.
2025-06-08 18:12:04,456:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\omslu\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['isSchool', 'rental_count',
                                             'return_count', 'apart',
                                             'closest_hospital_dist',
                                             'closest_convenience_dist',
                                             'closest_culture_dist', 'age_20s',
                                             'age_30s', 'age_40s',
                                             'crosswalk_count',
                                             'closest_cross_dist', 'bus_count'...
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2025-06-08 18:12:04,458:INFO:Creating final display dataframe.
2025-06-08 18:12:04,733:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target        risk_level
2                   Target type        Multiclass
3           Original data shape        (7806, 28)
4        Transformed data shape        (7806, 28)
5   Transformed train set shape        (5464, 28)
6    Transformed test set shape        (2342, 28)
7              Numeric features                27
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              d5ac
2025-06-08 18:12:04,957:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:04,964:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:05,145:INFO:Soft dependency imported: xgboost: 3.0.2
2025-06-08 18:12:05,145:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-08 18:12:05,145:INFO:setup() successfully completed in 2.77s...............
2025-06-08 18:12:12,826:INFO:Initializing compare_models()
2025-06-08 18:12:12,826:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-06-08 18:12:12,826:INFO:Checking exceptions
2025-06-08 18:12:12,847:INFO:Preparing display monitor
2025-06-08 18:12:12,867:INFO:Initializing Logistic Regression
2025-06-08 18:12:12,867:INFO:Total runtime is 0.0 minutes
2025-06-08 18:12:12,867:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:12,867:INFO:Initializing create_model()
2025-06-08 18:12:12,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:12,867:INFO:Checking exceptions
2025-06-08 18:12:12,867:INFO:Importing libraries
2025-06-08 18:12:12,867:INFO:Copying training dataset
2025-06-08 18:12:12,895:INFO:Defining folds
2025-06-08 18:12:12,895:INFO:Declaring metric variables
2025-06-08 18:12:12,895:INFO:Importing untrained model
2025-06-08 18:12:12,911:INFO:Logistic Regression Imported successfully
2025-06-08 18:12:12,912:INFO:Starting cross validation
2025-06-08 18:12:12,926:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:21,263:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,392:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,397:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,400:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,422:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,450:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:21,577:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,591:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:21,598:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,629:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,693:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,726:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:21,740:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:21,759:INFO:Calculating mean and std
2025-06-08 18:12:21,763:INFO:Creating metrics dataframe
2025-06-08 18:12:21,768:INFO:Uploading results into container
2025-06-08 18:12:21,768:INFO:Uploading model into container now
2025-06-08 18:12:21,770:INFO:_master_model_container: 1
2025-06-08 18:12:21,770:INFO:_display_container: 2
2025-06-08 18:12:21,771:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-08 18:12:21,771:INFO:create_model() successfully completed......................................
2025-06-08 18:12:21,939:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:21,939:INFO:Creating metrics dataframe
2025-06-08 18:12:21,956:INFO:Initializing K Neighbors Classifier
2025-06-08 18:12:21,956:INFO:Total runtime is 0.15147292216618854 minutes
2025-06-08 18:12:21,968:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:21,968:INFO:Initializing create_model()
2025-06-08 18:12:21,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:21,972:INFO:Checking exceptions
2025-06-08 18:12:21,972:INFO:Importing libraries
2025-06-08 18:12:21,972:INFO:Copying training dataset
2025-06-08 18:12:22,004:INFO:Defining folds
2025-06-08 18:12:22,004:INFO:Declaring metric variables
2025-06-08 18:12:22,012:INFO:Importing untrained model
2025-06-08 18:12:22,019:INFO:K Neighbors Classifier Imported successfully
2025-06-08 18:12:22,034:INFO:Starting cross validation
2025-06-08 18:12:22,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:28,123:INFO:Calculating mean and std
2025-06-08 18:12:28,126:INFO:Creating metrics dataframe
2025-06-08 18:12:28,126:INFO:Uploading results into container
2025-06-08 18:12:28,126:INFO:Uploading model into container now
2025-06-08 18:12:28,126:INFO:_master_model_container: 2
2025-06-08 18:12:28,126:INFO:_display_container: 2
2025-06-08 18:12:28,126:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-08 18:12:28,126:INFO:create_model() successfully completed......................................
2025-06-08 18:12:28,324:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:28,324:INFO:Creating metrics dataframe
2025-06-08 18:12:28,341:INFO:Initializing Naive Bayes
2025-06-08 18:12:28,341:INFO:Total runtime is 0.2578975160916646 minutes
2025-06-08 18:12:28,347:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:28,347:INFO:Initializing create_model()
2025-06-08 18:12:28,347:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:28,347:INFO:Checking exceptions
2025-06-08 18:12:28,347:INFO:Importing libraries
2025-06-08 18:12:28,347:INFO:Copying training dataset
2025-06-08 18:12:28,376:INFO:Defining folds
2025-06-08 18:12:28,376:INFO:Declaring metric variables
2025-06-08 18:12:28,384:INFO:Importing untrained model
2025-06-08 18:12:28,391:INFO:Naive Bayes Imported successfully
2025-06-08 18:12:28,403:INFO:Starting cross validation
2025-06-08 18:12:28,405:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:28,553:INFO:Calculating mean and std
2025-06-08 18:12:28,553:INFO:Creating metrics dataframe
2025-06-08 18:12:28,559:INFO:Uploading results into container
2025-06-08 18:12:28,559:INFO:Uploading model into container now
2025-06-08 18:12:28,559:INFO:_master_model_container: 3
2025-06-08 18:12:28,559:INFO:_display_container: 2
2025-06-08 18:12:28,559:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-08 18:12:28,559:INFO:create_model() successfully completed......................................
2025-06-08 18:12:28,748:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:28,748:INFO:Creating metrics dataframe
2025-06-08 18:12:28,764:INFO:Initializing Decision Tree Classifier
2025-06-08 18:12:28,764:INFO:Total runtime is 0.2649453322092692 minutes
2025-06-08 18:12:28,771:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:28,772:INFO:Initializing create_model()
2025-06-08 18:12:28,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:28,772:INFO:Checking exceptions
2025-06-08 18:12:28,772:INFO:Importing libraries
2025-06-08 18:12:28,772:INFO:Copying training dataset
2025-06-08 18:12:28,798:INFO:Defining folds
2025-06-08 18:12:28,799:INFO:Declaring metric variables
2025-06-08 18:12:28,806:INFO:Importing untrained model
2025-06-08 18:12:28,813:INFO:Decision Tree Classifier Imported successfully
2025-06-08 18:12:28,825:INFO:Starting cross validation
2025-06-08 18:12:28,827:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:29,191:INFO:Calculating mean and std
2025-06-08 18:12:29,191:INFO:Creating metrics dataframe
2025-06-08 18:12:29,196:INFO:Uploading results into container
2025-06-08 18:12:29,196:INFO:Uploading model into container now
2025-06-08 18:12:29,196:INFO:_master_model_container: 4
2025-06-08 18:12:29,196:INFO:_display_container: 2
2025-06-08 18:12:29,196:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-06-08 18:12:29,196:INFO:create_model() successfully completed......................................
2025-06-08 18:12:29,381:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:29,381:INFO:Creating metrics dataframe
2025-06-08 18:12:29,397:INFO:Initializing SVM - Linear Kernel
2025-06-08 18:12:29,397:INFO:Total runtime is 0.27549552520116166 minutes
2025-06-08 18:12:29,414:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:29,415:INFO:Initializing create_model()
2025-06-08 18:12:29,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:29,415:INFO:Checking exceptions
2025-06-08 18:12:29,416:INFO:Importing libraries
2025-06-08 18:12:29,416:INFO:Copying training dataset
2025-06-08 18:12:29,449:INFO:Defining folds
2025-06-08 18:12:29,450:INFO:Declaring metric variables
2025-06-08 18:12:29,461:INFO:Importing untrained model
2025-06-08 18:12:29,472:INFO:SVM - Linear Kernel Imported successfully
2025-06-08 18:12:29,486:INFO:Starting cross validation
2025-06-08 18:12:29,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:29,943:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:29,987:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,001:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,023:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,026:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,043:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,043:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,050:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,055:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,056:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,059:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,096:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,112:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,143:INFO:Calculating mean and std
2025-06-08 18:12:30,146:INFO:Creating metrics dataframe
2025-06-08 18:12:30,150:INFO:Uploading results into container
2025-06-08 18:12:30,151:INFO:Uploading model into container now
2025-06-08 18:12:30,152:INFO:_master_model_container: 5
2025-06-08 18:12:30,152:INFO:_display_container: 2
2025-06-08 18:12:30,153:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-08 18:12:30,153:INFO:create_model() successfully completed......................................
2025-06-08 18:12:30,328:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:30,328:INFO:Creating metrics dataframe
2025-06-08 18:12:30,347:INFO:Initializing Ridge Classifier
2025-06-08 18:12:30,347:INFO:Total runtime is 0.2913384000460306 minutes
2025-06-08 18:12:30,354:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:30,354:INFO:Initializing create_model()
2025-06-08 18:12:30,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:30,355:INFO:Checking exceptions
2025-06-08 18:12:30,355:INFO:Importing libraries
2025-06-08 18:12:30,355:INFO:Copying training dataset
2025-06-08 18:12:30,381:INFO:Defining folds
2025-06-08 18:12:30,381:INFO:Declaring metric variables
2025-06-08 18:12:30,389:INFO:Importing untrained model
2025-06-08 18:12:30,397:INFO:Ridge Classifier Imported successfully
2025-06-08 18:12:30,410:INFO:Starting cross validation
2025-06-08 18:12:30,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:30,481:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,502:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,513:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,524:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,524:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,530:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,530:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,530:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,532:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,534:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,534:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:30,538:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,542:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,542:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,544:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,548:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,548:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,550:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,550:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:30,572:INFO:Calculating mean and std
2025-06-08 18:12:30,572:INFO:Creating metrics dataframe
2025-06-08 18:12:30,577:INFO:Uploading results into container
2025-06-08 18:12:30,581:INFO:Uploading model into container now
2025-06-08 18:12:30,582:INFO:_master_model_container: 6
2025-06-08 18:12:30,582:INFO:_display_container: 2
2025-06-08 18:12:30,583:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-06-08 18:12:30,583:INFO:create_model() successfully completed......................................
2025-06-08 18:12:30,758:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:30,759:INFO:Creating metrics dataframe
2025-06-08 18:12:30,776:INFO:Initializing Random Forest Classifier
2025-06-08 18:12:30,776:INFO:Total runtime is 0.2984808882077534 minutes
2025-06-08 18:12:30,783:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:30,783:INFO:Initializing create_model()
2025-06-08 18:12:30,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:30,783:INFO:Checking exceptions
2025-06-08 18:12:30,783:INFO:Importing libraries
2025-06-08 18:12:30,783:INFO:Copying training dataset
2025-06-08 18:12:30,811:INFO:Defining folds
2025-06-08 18:12:30,811:INFO:Declaring metric variables
2025-06-08 18:12:30,818:INFO:Importing untrained model
2025-06-08 18:12:30,827:INFO:Random Forest Classifier Imported successfully
2025-06-08 18:12:30,840:INFO:Starting cross validation
2025-06-08 18:12:30,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:34,726:INFO:Calculating mean and std
2025-06-08 18:12:34,727:INFO:Creating metrics dataframe
2025-06-08 18:12:34,727:INFO:Uploading results into container
2025-06-08 18:12:34,727:INFO:Uploading model into container now
2025-06-08 18:12:34,727:INFO:_master_model_container: 7
2025-06-08 18:12:34,727:INFO:_display_container: 2
2025-06-08 18:12:34,727:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-06-08 18:12:34,727:INFO:create_model() successfully completed......................................
2025-06-08 18:12:34,893:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:34,893:INFO:Creating metrics dataframe
2025-06-08 18:12:34,910:INFO:Initializing Quadratic Discriminant Analysis
2025-06-08 18:12:34,910:INFO:Total runtime is 0.3673786481221516 minutes
2025-06-08 18:12:34,929:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:34,929:INFO:Initializing create_model()
2025-06-08 18:12:34,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:34,929:INFO:Checking exceptions
2025-06-08 18:12:34,929:INFO:Importing libraries
2025-06-08 18:12:34,929:INFO:Copying training dataset
2025-06-08 18:12:34,959:INFO:Defining folds
2025-06-08 18:12:34,960:INFO:Declaring metric variables
2025-06-08 18:12:34,967:INFO:Importing untrained model
2025-06-08 18:12:34,973:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-08 18:12:34,987:INFO:Starting cross validation
2025-06-08 18:12:34,990:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:35,042:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-08 18:12:35,045:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,048:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,073:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-06-08 18:12:35,089:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,093:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,102:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,106:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,106:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,112:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,116:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,125:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:35,157:INFO:Calculating mean and std
2025-06-08 18:12:35,160:INFO:Creating metrics dataframe
2025-06-08 18:12:35,164:INFO:Uploading results into container
2025-06-08 18:12:35,164:INFO:Uploading model into container now
2025-06-08 18:12:35,164:INFO:_master_model_container: 8
2025-06-08 18:12:35,166:INFO:_display_container: 2
2025-06-08 18:12:35,166:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-08 18:12:35,166:INFO:create_model() successfully completed......................................
2025-06-08 18:12:35,347:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:35,347:INFO:Creating metrics dataframe
2025-06-08 18:12:35,369:INFO:Initializing Ada Boost Classifier
2025-06-08 18:12:35,370:INFO:Total runtime is 0.3750496387481688 minutes
2025-06-08 18:12:35,379:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:35,379:INFO:Initializing create_model()
2025-06-08 18:12:35,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:35,380:INFO:Checking exceptions
2025-06-08 18:12:35,380:INFO:Importing libraries
2025-06-08 18:12:35,380:INFO:Copying training dataset
2025-06-08 18:12:35,411:INFO:Defining folds
2025-06-08 18:12:35,411:INFO:Declaring metric variables
2025-06-08 18:12:35,419:INFO:Importing untrained model
2025-06-08 18:12:35,426:INFO:Ada Boost Classifier Imported successfully
2025-06-08 18:12:35,430:INFO:Starting cross validation
2025-06-08 18:12:35,430:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:35,462:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,496:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,498:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,504:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,513:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,516:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,518:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,521:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,524:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:35,531:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-08 18:12:36,973:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,010:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,012:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,014:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,020:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,030:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,034:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:37,036:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,057:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,059:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,071:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:37,082:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:37,113:INFO:Calculating mean and std
2025-06-08 18:12:37,116:INFO:Creating metrics dataframe
2025-06-08 18:12:37,120:INFO:Uploading results into container
2025-06-08 18:12:37,121:INFO:Uploading model into container now
2025-06-08 18:12:37,122:INFO:_master_model_container: 9
2025-06-08 18:12:37,122:INFO:_display_container: 2
2025-06-08 18:12:37,122:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-06-08 18:12:37,123:INFO:create_model() successfully completed......................................
2025-06-08 18:12:37,305:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:37,306:INFO:Creating metrics dataframe
2025-06-08 18:12:37,329:INFO:Initializing Gradient Boosting Classifier
2025-06-08 18:12:37,329:INFO:Total runtime is 0.40770099957784006 minutes
2025-06-08 18:12:37,332:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:37,332:INFO:Initializing create_model()
2025-06-08 18:12:37,332:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:37,332:INFO:Checking exceptions
2025-06-08 18:12:37,332:INFO:Importing libraries
2025-06-08 18:12:37,332:INFO:Copying training dataset
2025-06-08 18:12:37,368:INFO:Defining folds
2025-06-08 18:12:37,369:INFO:Declaring metric variables
2025-06-08 18:12:37,376:INFO:Importing untrained model
2025-06-08 18:12:37,384:INFO:Gradient Boosting Classifier Imported successfully
2025-06-08 18:12:37,393:INFO:Starting cross validation
2025-06-08 18:12:37,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:54,327:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,443:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,443:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,461:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,461:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,544:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,544:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,560:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,593:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,756:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:54,793:INFO:Calculating mean and std
2025-06-08 18:12:54,797:INFO:Creating metrics dataframe
2025-06-08 18:12:54,800:INFO:Uploading results into container
2025-06-08 18:12:54,800:INFO:Uploading model into container now
2025-06-08 18:12:54,800:INFO:_master_model_container: 10
2025-06-08 18:12:54,800:INFO:_display_container: 2
2025-06-08 18:12:54,800:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-08 18:12:54,800:INFO:create_model() successfully completed......................................
2025-06-08 18:12:54,994:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:54,995:INFO:Creating metrics dataframe
2025-06-08 18:12:55,015:INFO:Initializing Linear Discriminant Analysis
2025-06-08 18:12:55,015:INFO:Total runtime is 0.702468430995941 minutes
2025-06-08 18:12:55,024:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:55,026:INFO:Initializing create_model()
2025-06-08 18:12:55,026:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:55,026:INFO:Checking exceptions
2025-06-08 18:12:55,026:INFO:Importing libraries
2025-06-08 18:12:55,026:INFO:Copying training dataset
2025-06-08 18:12:55,051:INFO:Defining folds
2025-06-08 18:12:55,051:INFO:Declaring metric variables
2025-06-08 18:12:55,061:INFO:Importing untrained model
2025-06-08 18:12:55,066:INFO:Linear Discriminant Analysis Imported successfully
2025-06-08 18:12:55,082:INFO:Starting cross validation
2025-06-08 18:12:55,083:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:55,166:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,184:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:55,202:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,206:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,209:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,218:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,218:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,220:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:55,224:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,231:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,233:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,234:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-08 18:12:55,244:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:12:55,259:INFO:Calculating mean and std
2025-06-08 18:12:55,260:INFO:Creating metrics dataframe
2025-06-08 18:12:55,265:INFO:Uploading results into container
2025-06-08 18:12:55,266:INFO:Uploading model into container now
2025-06-08 18:12:55,267:INFO:_master_model_container: 11
2025-06-08 18:12:55,267:INFO:_display_container: 2
2025-06-08 18:12:55,268:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-08 18:12:55,268:INFO:create_model() successfully completed......................................
2025-06-08 18:12:55,444:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:55,445:INFO:Creating metrics dataframe
2025-06-08 18:12:55,466:INFO:Initializing Extra Trees Classifier
2025-06-08 18:12:55,466:INFO:Total runtime is 0.709975763161977 minutes
2025-06-08 18:12:55,484:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:55,485:INFO:Initializing create_model()
2025-06-08 18:12:55,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:55,485:INFO:Checking exceptions
2025-06-08 18:12:55,485:INFO:Importing libraries
2025-06-08 18:12:55,485:INFO:Copying training dataset
2025-06-08 18:12:55,512:INFO:Defining folds
2025-06-08 18:12:55,512:INFO:Declaring metric variables
2025-06-08 18:12:55,522:INFO:Importing untrained model
2025-06-08 18:12:55,532:INFO:Extra Trees Classifier Imported successfully
2025-06-08 18:12:55,551:INFO:Starting cross validation
2025-06-08 18:12:55,553:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:12:57,564:INFO:Calculating mean and std
2025-06-08 18:12:57,568:INFO:Creating metrics dataframe
2025-06-08 18:12:57,572:INFO:Uploading results into container
2025-06-08 18:12:57,573:INFO:Uploading model into container now
2025-06-08 18:12:57,574:INFO:_master_model_container: 12
2025-06-08 18:12:57,574:INFO:_display_container: 2
2025-06-08 18:12:57,575:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-06-08 18:12:57,575:INFO:create_model() successfully completed......................................
2025-06-08 18:12:57,743:INFO:SubProcess create_model() end ==================================
2025-06-08 18:12:57,744:INFO:Creating metrics dataframe
2025-06-08 18:12:57,765:INFO:Initializing Extreme Gradient Boosting
2025-06-08 18:12:57,765:INFO:Total runtime is 0.74830265045166 minutes
2025-06-08 18:12:57,770:INFO:SubProcess create_model() called ==================================
2025-06-08 18:12:57,771:INFO:Initializing create_model()
2025-06-08 18:12:57,771:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:12:57,771:INFO:Checking exceptions
2025-06-08 18:12:57,771:INFO:Importing libraries
2025-06-08 18:12:57,771:INFO:Copying training dataset
2025-06-08 18:12:57,796:INFO:Defining folds
2025-06-08 18:12:57,797:INFO:Declaring metric variables
2025-06-08 18:12:57,803:INFO:Importing untrained model
2025-06-08 18:12:57,811:INFO:Extreme Gradient Boosting Imported successfully
2025-06-08 18:12:57,823:INFO:Starting cross validation
2025-06-08 18:12:57,824:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:13:00,423:INFO:Calculating mean and std
2025-06-08 18:13:00,429:INFO:Creating metrics dataframe
2025-06-08 18:13:00,429:INFO:Uploading results into container
2025-06-08 18:13:00,429:INFO:Uploading model into container now
2025-06-08 18:13:00,429:INFO:_master_model_container: 13
2025-06-08 18:13:00,429:INFO:_display_container: 2
2025-06-08 18:13:00,429:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-06-08 18:13:00,429:INFO:create_model() successfully completed......................................
2025-06-08 18:13:00,608:INFO:SubProcess create_model() end ==================================
2025-06-08 18:13:00,608:INFO:Creating metrics dataframe
2025-06-08 18:13:00,630:INFO:Initializing Light Gradient Boosting Machine
2025-06-08 18:13:00,630:INFO:Total runtime is 0.7960417628288268 minutes
2025-06-08 18:13:00,640:INFO:SubProcess create_model() called ==================================
2025-06-08 18:13:00,641:INFO:Initializing create_model()
2025-06-08 18:13:00,641:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:13:00,642:INFO:Checking exceptions
2025-06-08 18:13:00,643:INFO:Importing libraries
2025-06-08 18:13:00,643:INFO:Copying training dataset
2025-06-08 18:13:00,661:INFO:Defining folds
2025-06-08 18:13:00,666:INFO:Declaring metric variables
2025-06-08 18:13:00,675:INFO:Importing untrained model
2025-06-08 18:13:00,683:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-08 18:13:00,697:INFO:Starting cross validation
2025-06-08 18:13:00,698:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:13:17,211:INFO:Calculating mean and std
2025-06-08 18:13:17,213:INFO:Creating metrics dataframe
2025-06-08 18:13:17,219:INFO:Uploading results into container
2025-06-08 18:13:17,219:INFO:Uploading model into container now
2025-06-08 18:13:17,221:INFO:_master_model_container: 14
2025-06-08 18:13:17,221:INFO:_display_container: 2
2025-06-08 18:13:17,223:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-08 18:13:17,223:INFO:create_model() successfully completed......................................
2025-06-08 18:13:17,393:INFO:SubProcess create_model() end ==================================
2025-06-08 18:13:17,393:INFO:Creating metrics dataframe
2025-06-08 18:13:17,411:INFO:Initializing Dummy Classifier
2025-06-08 18:13:17,411:INFO:Total runtime is 1.0757302045822144 minutes
2025-06-08 18:13:17,422:INFO:SubProcess create_model() called ==================================
2025-06-08 18:13:17,423:INFO:Initializing create_model()
2025-06-08 18:13:17,423:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6EBD22F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:13:17,423:INFO:Checking exceptions
2025-06-08 18:13:17,423:INFO:Importing libraries
2025-06-08 18:13:17,423:INFO:Copying training dataset
2025-06-08 18:13:17,452:INFO:Defining folds
2025-06-08 18:13:17,453:INFO:Declaring metric variables
2025-06-08 18:13:17,462:INFO:Importing untrained model
2025-06-08 18:13:17,462:INFO:Dummy Classifier Imported successfully
2025-06-08 18:13:17,486:INFO:Starting cross validation
2025-06-08 18:13:17,487:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:13:17,550:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,586:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,590:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,601:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,601:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,601:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,607:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,607:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,614:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,618:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-08 18:13:17,643:INFO:Calculating mean and std
2025-06-08 18:13:17,647:INFO:Creating metrics dataframe
2025-06-08 18:13:17,651:INFO:Uploading results into container
2025-06-08 18:13:17,652:INFO:Uploading model into container now
2025-06-08 18:13:17,652:INFO:_master_model_container: 15
2025-06-08 18:13:17,652:INFO:_display_container: 2
2025-06-08 18:13:17,653:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-06-08 18:13:17,653:INFO:create_model() successfully completed......................................
2025-06-08 18:13:17,843:INFO:SubProcess create_model() end ==================================
2025-06-08 18:13:17,843:INFO:Creating metrics dataframe
2025-06-08 18:13:17,871:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-06-08 18:13:17,890:INFO:Initializing create_model()
2025-06-08 18:13:17,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:13:17,890:INFO:Checking exceptions
2025-06-08 18:13:17,894:INFO:Importing libraries
2025-06-08 18:13:17,894:INFO:Copying training dataset
2025-06-08 18:13:17,919:INFO:Defining folds
2025-06-08 18:13:17,919:INFO:Declaring metric variables
2025-06-08 18:13:17,919:INFO:Importing untrained model
2025-06-08 18:13:17,919:INFO:Declaring custom model
2025-06-08 18:13:17,921:INFO:Ridge Classifier Imported successfully
2025-06-08 18:13:17,922:INFO:Cross validation set to False
2025-06-08 18:13:17,922:INFO:Fitting Model
2025-06-08 18:13:17,948:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-06-08 18:13:17,948:INFO:create_model() successfully completed......................................
2025-06-08 18:13:18,235:INFO:_master_model_container: 15
2025-06-08 18:13:18,235:INFO:_display_container: 2
2025-06-08 18:13:18,236:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-06-08 18:13:18,236:INFO:compare_models() successfully completed......................................
2025-06-08 18:13:46,814:INFO:Initializing create_model()
2025-06-08 18:13:46,814:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': 'balanced'})
2025-06-08 18:13:46,814:INFO:Checking exceptions
2025-06-08 18:13:46,837:INFO:Importing libraries
2025-06-08 18:13:46,840:INFO:Copying training dataset
2025-06-08 18:13:46,860:INFO:Defining folds
2025-06-08 18:13:46,860:INFO:Declaring metric variables
2025-06-08 18:13:46,860:INFO:Importing untrained model
2025-06-08 18:13:46,877:INFO:Extreme Gradient Boosting Imported successfully
2025-06-08 18:13:46,896:INFO:Starting cross validation
2025-06-08 18:13:46,896:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:13:49,393:INFO:Calculating mean and std
2025-06-08 18:13:49,393:INFO:Creating metrics dataframe
2025-06-08 18:13:49,393:INFO:Finalizing model
2025-06-08 18:13:50,304:INFO:Uploading results into container
2025-06-08 18:13:50,306:INFO:Uploading model into container now
2025-06-08 18:13:50,321:INFO:_master_model_container: 16
2025-06-08 18:13:50,323:INFO:_display_container: 3
2025-06-08 18:13:50,323:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...)
2025-06-08 18:13:50,323:INFO:create_model() successfully completed......................................
2025-06-08 18:13:59,888:INFO:Initializing tune_model()
2025-06-08 18:13:59,888:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>)
2025-06-08 18:13:59,888:INFO:Checking exceptions
2025-06-08 18:13:59,914:INFO:Copying training dataset
2025-06-08 18:13:59,929:INFO:Checking base model
2025-06-08 18:13:59,930:INFO:Base model : Extreme Gradient Boosting
2025-06-08 18:13:59,930:INFO:Declaring metric variables
2025-06-08 18:13:59,943:INFO:Defining Hyperparameters
2025-06-08 18:14:00,125:INFO:Tuning with n_jobs=-1
2025-06-08 18:14:00,131:INFO:Initializing RandomizedSearchCV
2025-06-08 18:14:20,243:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__scale_pos_weight': 38.0, 'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__n_estimators': 60, 'actual_estimator__min_child_weight': 1, 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.4, 'actual_estimator__colsample_bytree': 0.7}
2025-06-08 18:14:20,245:INFO:Hyperparameter search completed
2025-06-08 18:14:20,245:INFO:SubProcess create_model() called ==================================
2025-06-08 18:14:20,245:INFO:Initializing create_model()
2025-06-08 18:14:20,245:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000025A6D075060>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'scale_pos_weight': 38.0, 'reg_lambda': 0.7, 'reg_alpha': 0.001, 'n_estimators': 60, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.4, 'colsample_bytree': 0.7})
2025-06-08 18:14:20,245:INFO:Checking exceptions
2025-06-08 18:14:20,245:INFO:Importing libraries
2025-06-08 18:14:20,245:INFO:Copying training dataset
2025-06-08 18:14:20,279:INFO:Defining folds
2025-06-08 18:14:20,279:INFO:Declaring metric variables
2025-06-08 18:14:20,287:INFO:Importing untrained model
2025-06-08 18:14:20,287:INFO:Declaring custom model
2025-06-08 18:14:20,296:INFO:Extreme Gradient Boosting Imported successfully
2025-06-08 18:14:20,309:INFO:Starting cross validation
2025-06-08 18:14:20,312:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:14:21,842:INFO:Calculating mean and std
2025-06-08 18:14:21,845:INFO:Creating metrics dataframe
2025-06-08 18:14:21,857:INFO:Finalizing model
2025-06-08 18:14:22,393:INFO:Uploading results into container
2025-06-08 18:14:22,395:INFO:Uploading model into container now
2025-06-08 18:14:22,396:INFO:_master_model_container: 17
2025-06-08 18:14:22,396:INFO:_display_container: 4
2025-06-08 18:14:22,396:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...)
2025-06-08 18:14:22,396:INFO:create_model() successfully completed......................................
2025-06-08 18:14:22,557:INFO:SubProcess create_model() end ==================================
2025-06-08 18:14:22,557:INFO:choose_better activated
2025-06-08 18:14:22,573:INFO:SubProcess create_model() called ==================================
2025-06-08 18:14:22,573:INFO:Initializing create_model()
2025-06-08 18:14:22,573:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:14:22,573:INFO:Checking exceptions
2025-06-08 18:14:22,573:INFO:Importing libraries
2025-06-08 18:14:22,573:INFO:Copying training dataset
2025-06-08 18:14:22,594:INFO:Defining folds
2025-06-08 18:14:22,594:INFO:Declaring metric variables
2025-06-08 18:14:22,594:INFO:Importing untrained model
2025-06-08 18:14:22,594:INFO:Declaring custom model
2025-06-08 18:14:22,594:INFO:Extreme Gradient Boosting Imported successfully
2025-06-08 18:14:22,610:INFO:Starting cross validation
2025-06-08 18:14:22,611:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-08 18:14:26,759:INFO:Calculating mean and std
2025-06-08 18:14:26,760:INFO:Creating metrics dataframe
2025-06-08 18:14:26,760:INFO:Finalizing model
2025-06-08 18:14:27,830:INFO:Uploading results into container
2025-06-08 18:14:27,835:INFO:Uploading model into container now
2025-06-08 18:14:27,835:INFO:_master_model_container: 18
2025-06-08 18:14:27,837:INFO:_display_container: 5
2025-06-08 18:14:27,837:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...)
2025-06-08 18:14:27,837:INFO:create_model() successfully completed......................................
2025-06-08 18:14:27,993:INFO:SubProcess create_model() end ==================================
2025-06-08 18:14:27,993:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=None, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1, ...) result for F1 is 0.7584
2025-06-08 18:14:27,993:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...) result for F1 is 0.7624
2025-06-08 18:14:27,993:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...) is best model
2025-06-08 18:14:27,993:INFO:choose_better completed
2025-06-08 18:14:28,026:INFO:_master_model_container: 18
2025-06-08 18:14:28,026:INFO:_display_container: 4
2025-06-08 18:14:28,033:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...)
2025-06-08 18:14:28,033:INFO:tune_model() successfully completed......................................
2025-06-08 18:14:59,280:INFO:Initializing finalize_model()
2025-06-08 18:14:59,281:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-06-08 18:14:59,282:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...)
2025-06-08 18:14:59,290:INFO:Initializing create_model()
2025-06-08 18:14:59,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              class_weight='balanced', colsample_bylevel=None,
              colsample_bynode=None, colsample_bytree=0.7, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.4, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=60, n_jobs=-1, ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-06-08 18:14:59,290:INFO:Checking exceptions
2025-06-08 18:14:59,292:INFO:Importing libraries
2025-06-08 18:14:59,292:INFO:Copying training dataset
2025-06-08 18:14:59,293:INFO:Defining folds
2025-06-08 18:14:59,293:INFO:Declaring metric variables
2025-06-08 18:14:59,293:INFO:Importing untrained model
2025-06-08 18:14:59,293:INFO:Declaring custom model
2025-06-08 18:14:59,294:INFO:Extreme Gradient Boosting Imported successfully
2025-06-08 18:14:59,295:INFO:Cross validation set to False
2025-06-08 18:14:59,295:INFO:Fitting Model
2025-06-08 18:15:00,023:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['isSchool', 'rental_count',
                                             'return_count', 'apart',
                                             'closest_hospital_dist',
                                             'closest_convenience_dist',
                                             'closest_culture_dist', 'age_20s',
                                             'age_30s', 'age_40s',
                                             'crosswalk_count',
                                             'closest_cross_dist', 'bus_count',
                                             'closest_bus_dist', 'subway_count',
                                             'clos...
                               feature_types=None, feature_weights=None,
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.4,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=6, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=60, n_jobs=-1, ...))],
         verbose=False)
2025-06-08 18:15:00,023:INFO:create_model() successfully completed......................................
2025-06-08 18:15:00,207:INFO:_master_model_container: 18
2025-06-08 18:15:00,208:INFO:_display_container: 4
2025-06-08 18:15:00,223:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['isSchool', 'rental_count',
                                             'return_count', 'apart',
                                             'closest_hospital_dist',
                                             'closest_convenience_dist',
                                             'closest_culture_dist', 'age_20s',
                                             'age_30s', 'age_40s',
                                             'crosswalk_count',
                                             'closest_cross_dist', 'bus_count',
                                             'closest_bus_dist', 'subway_count',
                                             'clos...
                               feature_types=None, feature_weights=None,
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.4,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=6, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=60, n_jobs=-1, ...))],
         verbose=False)
2025-06-08 18:15:00,223:INFO:finalize_model() successfully completed......................................
2025-06-08 18:15:00,404:INFO:Initializing predict_model()
2025-06-08 18:15:00,404:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['isSchool', 'rental_count',
                                             'return_count', 'apart',
                                             'closest_hospital_dist',
                                             'closest_convenience_dist',
                                             'closest_culture_dist', 'age_20s',
                                             'age_30s', 'age_40s',
                                             'crosswalk_count',
                                             'closest_cross_dist', 'bus_count',
                                             'closest_bus_dist', 'subway_count',
                                             'clos...
                               feature_types=None, feature_weights=None,
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.4,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=6, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=60, n_jobs=-1, ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025A6EB3BD00>)
2025-06-08 18:15:00,404:INFO:Checking exceptions
2025-06-08 18:15:00,404:INFO:Preloading libraries
2025-06-08 18:15:00,408:INFO:Set up data.
2025-06-08 18:15:00,429:INFO:Set up index.
2025-06-08 18:15:35,099:INFO:Initializing get_config()
2025-06-08 18:15:35,101:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000025A6EABE230>, variable=X_train)
2025-06-08 18:15:35,102:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2025-06-08 18:15:35,104:WARNING:C:\Users\omslu\anaconda3\envs\pycaret_env\lib\site-packages\pycaret\internal\pycaret_experiment\pycaret_experiment.py:321: UserWarning: Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
  warnings.warn(msg)  # print on screen

2025-06-08 18:15:35,118:INFO:Variable:  returned as        isSchool  rental_count  return_count     apart  closest_hospital_dist  \
1526          0      0.000000      0.000000  0.000000               6.753224   
1813          1      4.577036      4.658558  0.000000               4.484650   
19279         0      0.000000      0.000000  0.000000               5.147014   
2599          1      4.934242      4.984048  6.028278               3.300803   
1936          0      0.000000      0.000000  0.000000               5.682774   
...         ...           ...           ...       ...                    ...   
32376         0      0.000000      0.000000  0.000000               4.301848   
28123         0      1.758970      1.678001  0.000000               4.374832   
31758         0      0.000000      0.000000  0.000000               5.541237   
1211          0      4.366176      4.369448  7.085901               5.738585   
20449         0      3.146173      3.154399  4.927254               4.933111   

       closest_convenience_dist  closest_culture_dist    age_20s    age_30s  \
1526                   6.397195              7.051930   9.029681   9.354688   
1813                   3.827818              6.605085   8.250020   8.318323   
19279                  4.320978              5.812755   7.517683   7.548420   
2599                   3.336692              6.248655   8.546797   8.831818   
1936                   4.713324              6.472786   9.029681   9.354688   
...                         ...                   ...        ...        ...   
32376                  3.340475              6.535544   7.419641   7.571612   
28123                  4.616723              5.404635  10.063892  10.244259   
31758                  5.374055              5.460620   7.862272   7.596469   
1211                   4.332340              6.986263   8.438911   8.746964   
20449                  3.861991              5.212646   8.476239   8.843242   

        age_40s  ...  bus_avg_alight  subway_avg_board  subway_avg_alight  \
1526   9.071215  ...        0.000000               0.0                0.0   
1813   8.673134  ...        0.000000               0.0                0.0   
19279  7.875865  ...        0.000000               0.0                0.0   
2599   8.748407  ...        7.465909               0.0                0.0   
1936   9.071215  ...        0.000000               0.0                0.0   
...         ...  ...             ...               ...                ...   
32376  7.758269  ...        0.000000               0.0                0.0   
28123  9.943179  ...        0.000000               0.0                0.0   
31758  7.718918  ...        0.000000               0.0                0.0   
1211   8.809739  ...        0.000000               0.0                0.0   
20449  8.811502  ...        0.000000               0.0                0.0   

          area20     area30     area40  wp_area20  wp_area30  wp_area40  \
1526    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   
1813   11.845654  11.870794  12.259103   3.332205   4.471639   5.096813   
19279   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   
2599    8.865947   9.152208   8.874273   3.917011   4.442651   4.491441   
1936   10.282943  10.599394  10.276731   3.663562   4.314149   4.442651   
...          ...        ...        ...        ...        ...        ...   
32376   8.422223   8.491619   8.631816   0.000000   1.098612   1.791759   
28123  12.798146  13.151178  13.023378   8.119249   8.578476   8.351434   
31758  10.203527   9.996284   9.987645   0.810930   0.916291   1.558145   
1211   10.836955  11.220720  11.535294   4.997212   5.669881   5.476463   
20449  10.955628  11.470751  11.502522   7.669145   7.822044   7.524696   

       isCommercial  
1526              0  
1813              1  
19279             1  
2599              1  
1936              1  
...             ...  
32376             1  
28123             1  
31758             1  
1211              1  
20449             0  

[5464 rows x 27 columns]
2025-06-08 18:15:35,118:INFO:get_config() successfully completed......................................
2025-06-08 18:15:39,951:WARNING:C:\Users\omslu\AppData\Local\Temp\ipykernel_996\544866967.py:5: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap.summary_plot(shap_values.values[:, :, class_idx], X_test)

2025-06-08 18:15:43,740:WARNING:C:\Users\omslu\AppData\Local\Temp\ipykernel_996\544866967.py:5: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap.summary_plot(shap_values.values[:, :, class_idx], X_test)

2025-06-08 18:15:47,703:WARNING:C:\Users\omslu\AppData\Local\Temp\ipykernel_996\544866967.py:5: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap.summary_plot(shap_values.values[:, :, class_idx], X_test)

